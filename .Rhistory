ee$Image$addBands(shadow_direction(image = s2_img))
# 4.5 IPL_UV algorithm ... exist enough images?
IPL_multitemporal_cloud_logical <- ee_upl_cloud_logical(
sen2id = basename(s2_id),
roi =  s2_img$geometry()
)
# 4.6 If IPL_multitemporal_cloud_logical is TRUE add to the EE dataset
if (IPL_multitemporal_cloud_logical) {
IPL_multitemporal_cloud <- ee_upl_cloud(
sen2id = basename(s2_id),
roi =  s2_img$geometry()
) %>% ee$Image$unmask(-99)
s2_fullinfo <- s2_fullinfo %>%
ee$Image$addBands(IPL_multitemporal_cloud)
}
# 4.7 Create a 511x511 tile (list -> data_frame -> sp -> raster)
band_names <- c(s2_fullinfo$bandNames()$getInfo(), "x", "y")
s2_img_array <- s2_fullinfo$addBands(s1_img) %>%
ee$Image$addBands(ee$Image$pixelCoordinates(projection = crs_kernel)) %>%
ee$Image$neighborhoodToArray(
kernel = ee$Kernel$rectangle(kernel_size[1], kernel_size[2], "pixels")
) %>%
ee$Image$sampleRegions(ee$FeatureCollection(ee_point),
projection = crs_kernel,
scale = 10) %>%
ee$FeatureCollection$getInfo()
extract_fn <- function(x) as.numeric(unlist(s2_img_array$features[[1]]$properties[x]))
image_as_df <- do.call(cbind,lapply(band_names, extract_fn))
colnames(image_as_df) <- band_names
image_as_tibble <- as_tibble(image_as_df)
coordinates(image_as_tibble) <- ~x+y
sf_to_stack <- function(x) rasterFromXYZ(image_as_tibble[x])
final_stack <- stack(lapply(names(image_as_tibble), sf_to_stack))
crs(final_stack) <- st_crs(crs_kernel)$proj4string
# 4.8 Add 360 if shadow direction is negative
final_stack[[24]] <- fix_shadow_direction(final_stack[[24]])
# 4.9 Estimate dataset_values
## s1_area_per
s1_raster_values <- getValues(final_stack[[16]])
na_pixels <- sum(is.na(s1_raster_values))
n99_pixels <- sum(s1_raster_values == -99, na.rm = TRUE)
s1_area <- 100 - (na_pixels + n99_pixels)/ncell(s1_raster_values)*100
s1_area_per[counter + 1] <- s1_area
## landuse
land_use[counter + 1] <- Modes(getValues(final_stack[[23]]))
## elevation
elevation[counter + 1] <- mean(getValues(final_stack[[22]]))
## shadow direction
shadow_dir[counter + 1] <- mean(getValues(final_stack[[24]]))
# 4.8 Prepare folders for iris
output_final_d <- sprintf("%s/dataset", output_final)
output_final_folder <- sprintf("%s/dataset/%s/%s", output_final, point_name, basename(s2_id))
metadata_main <- sprintf("%s/dataset/%s/cloud_segmentation_%s.json", output_final, point_name, point_name)
metadata_spec <- sprintf("%s/dataset/%s/%s/metadata.json", output_final, point_name, basename(s2_id))
dir.create(sprintf("%s/input", output_final_folder), showWarnings = FALSE, recursive = TRUE)
dir.create(sprintf("%s/target", output_final_folder), showWarnings = FALSE, recursive = TRUE)
dir.create(sprintf("%s/thumbnails", output_final_folder), showWarnings = FALSE, recursive = TRUE)
# 4.9 Create cloud-segmentation.json (main file in iris software)
metadata_main <- sprintf("%s/dataset/%s/cloud_segmentation_%s.json", output_final, point_name, point_name)
if (counter == 0) {
ee_create_cloudseg(path = metadata_main)
}
# 4.10 Users now have userID:password (lab:lab)
if (counter == 0) {
lab_lab_user(path = dirname(metadata_main), point_name = point_name)
}
# 4.11 Generate a script to upload results to Google Drive database
if (counter == 0) {
generate_script(dirname(metadata_main))
}
# 4.12 Save all features inside the input folder
bandnames <- c(paste0("B",1:8), "B8A", paste0("B", 9:12), "CDI", "VV", "VH", "angle", "elevation", "landuse", "cloudshadow_direction")
# 4.14 Save input values
input_data <- raster::stack(
final_stack[[1:13]]/10000, final_stack[[14]], final_stack[[15:17]], final_stack[[22:24]]
)
input_spec <- sprintf("%s/input/%s.tif", output_final_folder, bandnames)
lapply(1:20, function(x) writeRaster(input_data[[x]], input_spec[x], overwrite = TRUE))
# 4.15 Save target values
# 18-19 -> cmask_s2cloudness| cmask_s2cloudness_reclass (0,1)
# 20-21 -> cmask_sen2cor | cmask_sen2cor_reclass (0,1,2)
# 25 -> IPL_cloudmask_reclass
create_target_raster(
final_stack = final_stack,
IPL_multitemporal_cloud_logical = IPL_multitemporal_cloud_logical,
output_final_folder = output_final_folder
)
# 4.16 Create metadata.json for each file
ee_create_metadata(
id = basename(s2_id),
point = c(jsonfile_r$y, jsonfile_r$x),
path = metadata_spec
)
# 4.17 Create a thumbnail
ee_generate_thumbnail(
s2_id = s2_id,
final_stack =  final_stack,
crs_kernel =  crs_kernel,
output_final_folder = output_final_folder
)
counter <- counter + 1
}
# point_metadata
df_final <- data_frame(
id = sprintf("%s_%02d", point_name, 1:5),
labeler = sp_db[row_position,]$labeler,
type = sp_db[row_position,]$label,
difficulty = NA,
sen2_id = s2_ids,
sen2_date = s2_dates,
sen1_id = s1_ids,
s1_date = s1_dates,
sen1_area = s1_area_per,
land_use = land_use,
elevation = elevation,
shadow_dir = shadow_dir,
split = NA,
state = FALSE,
evaluation_I = sp_db[row_position,]$validator,
evaluation_II = labelers_names[!(labelers_names %in% c(sp_db[row_position,]$validator, sp_db[row_position,]$labeler))],
evaluation_Expert = FALSE
)
write_csv(
x = df_final,
file = sprintf("%s/%s_metadata.csv", dirname(metadata_main), point_name)
)
# 5. Save geometry
roi <- extent(final_stack[[1]]) %>%
st_bbox() %>%
st_as_sfc()
st_crs(roi) <- crs_kernel
write_sf(roi, sprintf("%s/%s.gpkg", dirname(output_final_folder), point_name))
}
mean(getValues(final_stack[[22]]))
s1_date
s1_dates[counter + 1] <- s1_date
s1_dates
s1_date
s1_date %>% as.character()
s1_dates[counter + 1] <- s1_date %>% as.character()
s1_dates
#'
#' Create several thumbnail to find the clouds closest to the theoretical probabilities
#' All sentinel2 images have a Sentinel1 pair with no more than 2.5 days of delay.
#'
#' @param n_images  Numeric. Number of images to download, if n_images = "max" the
#' function will download all the available images.
#' @param kernel_size Size of the kernel.
#' @param data_range Range of dates to obtain images.
#' @param output Folder where to save the results.
#' @noRd
select_dataset_thumbnail_creator <- function(cloudsen2_row,
n_images = "max",
kernel_size = c(255, 255),
data_range = c("2018-01-01", "2020-07-31"),
output = "results/") {
# 1. Create output directory
dir.create(output, showWarnings = FALSE)
# 2. Create a point which represent the center of the chip (from local to Earth Engine)
point <- ee$Geometry$Point(cloudsen2_row$geometry[[1]])
# 3. Create a S2 ImageCollection.
s2Sr <- ee$ImageCollection("COPERNICUS/S2_SR") %>%
ee$ImageCollection$filterBounds(point) %>%
ee$ImageCollection$filterDate(data_range[1], data_range[2])
# 4. Create a S1 ImageCollection.
data_range2 <- c(as.Date(data_range[1]) - 5, as.Date(data_range[2]) + 5) %>% as.character()
s1_grd <- ee$ImageCollection("COPERNICUS/S1_GRD") %>%
ee$ImageCollection$filterBounds(point) %>%
ee$ImageCollection$filterDate(data_range2[1], data_range2[2]) %>%
# Filter to get images with VV and VH dual polarization.
ee$ImageCollection$filter(ee$Filter$listContains("transmitterReceiverPolarisation", "VV")) %>%
ee$ImageCollection$filter(ee$Filter$listContains('transmitterReceiverPolarisation', "VH")) %>%
# Filter to get images collected in interferometric wide swath mode.
ee$ImageCollection$filter(ee$Filter$eq("instrumentMode", "IW"))
# 5. Get dates from all the images which are part of S1 and S2 ImageCollection.
s2_dates <- ee_get_date_ic(s2Sr) %>% as_tibble()
s1_dates <- tryCatch(
expr = ee_get_date_ic(s1_grd) %>% as_tibble(),
error = function(e) stop("Sentinel-1 no images ... Please Report!: ", e)
)
sx_fx <- function(x) min(abs(s2_dates$time_start[x] - s1_dates$time_start))
mindays <- sapply(seq_len(nrow(s2_dates)), sx_fx)
valid_s2 <- s2_dates[mindays < 2.5,] # Pick up images with no more than 2.5 days of delay
# Display the number of available images with an S2/S1 pair
message(sprintf("Number of images: %s", nrow(valid_s2)))
if (n_images == "max") {
n_images <- nrow(valid_s2)
}
# 6. Get the CRS of this specific point (5 images)
img_crs <- s2Sr$first()$select(0)$projection()$getInfo()[["crs"]]
# 7. shuffle valid images
images_position <- sample(nrow(valid_s2), nrow(valid_s2))
# if (length(images_position) < 50) {
#   warning("Insufficient number of images ... PLEASE REPORT!")
# }
# 8. Create a folder to save results
dir_id <- sprintf("%s/point_%04d",output, cloudsen2_row$id)
dir.create(dir_id, showWarnings = FALSE)
# 9. Download all the image thumbnails
counter <- 0
for (r_index in images_position) {
if (counter == n_images) {
break
}
# 9.1 Select the image
img_to_download <- ee$Image(valid_s2$id[r_index])
img_id <- basename(valid_s2$id[r_index])
# 9.2 Download the S2 thumbnail image (as a json)
s2_img_array <- img_to_download %>%
ee$Image$select(c("B4", "B3", "B2")) %>%
ee$Image$addBands(ee$Image$pixelCoordinates(projection = img_crs)) %>%
# ee$Image$reproject(crs = img_crs, scale = 10) %>%
ee$Image$neighborhoodToArray(
kernel = ee$Kernel$rectangle(kernel_size[1], kernel_size[2], "pixels")
) %>%
ee$Image$sampleRegions(ee$FeatureCollection(point),
projection = img_crs,
scale = 10) %>%
ee$FeatureCollection$getInfo()
# Some image have FULL NA values if that occurs skip
band_names <- try(
expr = names(s2_img_array$features[[1]]$properties),
silent = TRUE
)
if (class(band_names) == "try-error") {
next
}
message(
sprintf("Processing point [%s] image [%s] ... please wait",
cloudsen2_row$id, counter)
)
# 9.3 From list to data_frame
extract_fn <- function(x) as.numeric(unlist(s2_img_array$features[[1]]$properties[x]))
image_as_df <- do.call(cbind,lapply(band_names, extract_fn))
colnames(image_as_df) <- band_names
image_as_tibble <- as_tibble(image_as_df)
# If all values in the image are zero, skip
if (sum(image_as_tibble[["B2"]] == 0) > 0) {
next
} else {
counter <- counter + 1
}
# 9.4 From data_frame to sp; From sp to raster
coordinates(image_as_tibble) <- ~x+y
sf_to_stack <- function(x) rasterFromXYZ(image_as_tibble[x])
final_stack <- stack(lapply(names(image_as_tibble), sf_to_stack))
# 9.4 Create a plot and download
crs(final_stack) <- st_crs(img_crs)$proj4string
png(sprintf("%s/%s.png", dir_id, img_id), 1000, 1000)
max_value <- max(maxValue(final_stack))
plotRGB(final_stack/max_value, r = 3, g = 2, b = 1, scale = 1)
dev.off()
}
# 10. Create the metadata.json file
metadata_dataset_creator(
cloudsen2_row = cloudsen2_row,
output = output
)
}
# 1. Libraries
library(googleCloudStorageR)
library(googledrive)
library(reticulate)
library(rasterVis)
library(tidyverse)
require(gridExtra)
library(jsonlite)
library(mapview)
library(mapedit)
library(raster)
library(scales)
library(mmand)
library(stars)
library(purrr)
library(grid)
library(Orcs)
library(rgee)
library(png)
library(sf)
library(sp)
# 2. Initialize Earth Engine
ee_Initialize("csaybar", drive = TRUE, gcs = TRUE)
source("src/utils.R")
ee_cloud <- import("ee_ipl_uv")
# 3. Load points with desired cloud average (after run point_creator.R)
local_cloudsen2_points <- read_sf("data/cloudsen2_potential_points.geojson")
local_cloudsen2_points
# 3. Load points with desired cloud average (after run point_creator.R)
local_cloudsen2_points <- read_sf("data/cloudsen2_potential_points.geojson")
# 8. Validation
drive_jsonfile <- drive_ls(
path = as_id("1fBGAjZkjPEpPr0p7c-LtJmfbLq3s87RK")
)
# 8. Validation
drive_jsonfile <- drive_ls(
path = as_id("1fBGAjZkjPEpPr0p7c-LtJmfbLq3s87RK"),
n_max = 10
)
drive_jsonfile
# 8. Validation
drive_jsonfile <- drive_ls(
path = as_id("1fBGAjZkjPEpPr0p7c-LtJmfbLq3s87RK"),
n_max = 15
)
set.seed(100)
jsonfiles <- drive_jsonfile$name[sample(length(drive_jsonfile$name), 15)]
jsonfiles
jsonfile_f <- search_metajson(pattern = jsonfile, clean = FALSE)
jsonfiles
jsonfile <- "metadata_2040.json"
jsonfile
jsonfile_f <- search_metajson(pattern = jsonfile, clean = FALSE)
jsonfile_f
dataset_creator_chips(
jsonfile = jsonfile_f,
sp_db = local_cloudsen2_points,
output_final = "/home/csaybar/Desktop/cloudsen12"
)
row_position
point_name
point_name <- paste0("point_", gsub("[a-zA-Z]|_|\\.","", basename(jsonfile)))
point_name
point_name
row_position <- gsub("point_", "", point_name)
row_position
row_position <- gsub("point_", "", point_name) %>% as.numeric()
row_position
sp_db = local_cloudsen2_points
sp_db[row_position,]$labeler
sp_db[row_position,]$label
#' Main function to create CLOUDSEN12 data
#'
#' This function is used to download all the images.
#'
#' @param jsonfile metadata (*.json) Extract files from Roy Drive.
#' @param kernel_size Size of the kernel.
#' @param output_final Folder where to save the results.
#'
dataset_creator_chips <- function(jsonfile,
sp_db,
kernel_size = c(255, 255),
output_final = "cloudsen12/") {
point_name <- paste0("point_", gsub("[a-zA-Z]|_|\\.","", basename(jsonfile)))
# 1. Read JSON file
jsonfile_r <- jsonlite::read_json(jsonfile)
# 2. Identify all the S2 images
s2_idsposition <- which(sapply(strsplit(names(jsonfile_r), "_"), length) == 3)
s2_ids <- sprintf("COPERNICUS/S2/%s", names(jsonfile_r)[s2_idsposition])
# 3. Create a st_point representing the center of the tile (255x255)
st_point <- st_sfc(geometry = st_point(c(jsonfile_r$x, jsonfile_r$y)), crs = 4326)
crs_kernel <- ee$Image(s2_ids[1])$select(0)$projection()$getInfo()$crs
point_utm <- st_transform(st_point, crs_kernel)
ee_point <- ee$Geometry$Point(point_utm[[1]], proj = crs_kernel)
# 4. Download each image for the specified point
counter <- 0
s2_dates <- rep(NA,5)
s1_dates <- rep(NA,5)
s1_ids <- rep(NA,5)
s1_area_per <- rep(NA,5)
land_use <- rep(NA,5)
elevation <- rep(NA,5)
shadow_dir <- rep(NA,5)
for (s2_id in s2_ids) {
message(sprintf("Downloading: %s", s2_id))
# 4.1 S2 ID and dates
s2_img <- ee$Image(s2_id)
s2_date <- ee_get_date_img(s2_img)[["time_start"]]
s2_dates[counter + 1] <- s2_date %>% as.character()
# 4.2 S1 ID
s1_id <- ee_get_s1(point = ee_point, s2_date = s2_date)
s1_img <- ee$Image(s1_id)
s1_date <- ee_get_date_img(s1_img)[["time_start"]]
s1_ids[counter + 1] <- s1_id
s1_dates[counter + 1] <- s1_date %>% as.character()
# 4.3 Create an Image collection with S2, S1 and cloud mask information
s2_s1_img <- ee_merge_s2_full(s2_id, s1_id, s2_date)
# 4.4 Add the shadow direction
s2_fullinfo <- s2_s1_img %>%
ee$Image$addBands(shadow_direction(image = s2_img))
# 4.5 IPL_UV algorithm ... exist enough images?
IPL_multitemporal_cloud_logical <- ee_upl_cloud_logical(
sen2id = basename(s2_id),
roi =  s2_img$geometry()
)
# 4.6 If IPL_multitemporal_cloud_logical is TRUE add to the EE dataset
if (IPL_multitemporal_cloud_logical) {
IPL_multitemporal_cloud <- ee_upl_cloud(
sen2id = basename(s2_id),
roi =  s2_img$geometry()
) %>% ee$Image$unmask(-99)
s2_fullinfo <- s2_fullinfo %>%
ee$Image$addBands(IPL_multitemporal_cloud)
}
# 4.7 Create a 511x511 tile (list -> data_frame -> sp -> raster)
band_names <- c(s2_fullinfo$bandNames()$getInfo(), "x", "y")
s2_img_array <- s2_fullinfo$addBands(s1_img) %>%
ee$Image$addBands(ee$Image$pixelCoordinates(projection = crs_kernel)) %>%
ee$Image$neighborhoodToArray(
kernel = ee$Kernel$rectangle(kernel_size[1], kernel_size[2], "pixels")
) %>%
ee$Image$sampleRegions(ee$FeatureCollection(ee_point),
projection = crs_kernel,
scale = 10) %>%
ee$FeatureCollection$getInfo()
extract_fn <- function(x) as.numeric(unlist(s2_img_array$features[[1]]$properties[x]))
image_as_df <- do.call(cbind,lapply(band_names, extract_fn))
colnames(image_as_df) <- band_names
image_as_tibble <- as_tibble(image_as_df)
coordinates(image_as_tibble) <- ~x+y
sf_to_stack <- function(x) rasterFromXYZ(image_as_tibble[x])
final_stack <- stack(lapply(names(image_as_tibble), sf_to_stack))
crs(final_stack) <- st_crs(crs_kernel)$proj4string
# 4.8 Add 360 if shadow direction is negative
final_stack[[24]] <- fix_shadow_direction(final_stack[[24]])
# 4.9 Estimate dataset_values
## s1_area_per
s1_raster_values <- getValues(final_stack[[16]])
na_pixels <- sum(is.na(s1_raster_values))
n99_pixels <- sum(s1_raster_values == -99, na.rm = TRUE)
s1_area <- 100 - (na_pixels + n99_pixels)/ncell(s1_raster_values)*100
s1_area_per[counter + 1] <- round(s1_area, 2)
## landuse
land_use[counter + 1] <- Modes(getValues(final_stack[[23]]))
## elevation
elevation[counter + 1] <- round(mean(getValues(final_stack[[22]])), 2)
## shadow direction
shadow_dir[counter + 1] <- round(mean(getValues(final_stack[[24]])), 2)
# 4.8 Prepare folders for iris
output_final_d <- sprintf("%s/dataset", output_final)
output_final_folder <- sprintf("%s/dataset/%s/%s", output_final, point_name, basename(s2_id))
metadata_main <- sprintf("%s/dataset/%s/cloud_segmentation_%s.json", output_final, point_name, point_name)
metadata_spec <- sprintf("%s/dataset/%s/%s/metadata.json", output_final, point_name, basename(s2_id))
dir.create(sprintf("%s/input", output_final_folder), showWarnings = FALSE, recursive = TRUE)
dir.create(sprintf("%s/target", output_final_folder), showWarnings = FALSE, recursive = TRUE)
dir.create(sprintf("%s/thumbnails", output_final_folder), showWarnings = FALSE, recursive = TRUE)
# 4.9 Create cloud-segmentation.json (main file in iris software)
metadata_main <- sprintf("%s/dataset/%s/cloud_segmentation_%s.json", output_final, point_name, point_name)
if (counter == 0) {
ee_create_cloudseg(path = metadata_main)
}
# 4.10 Users now have userID:password (lab:lab)
if (counter == 0) {
lab_lab_user(path = dirname(metadata_main), point_name = point_name)
}
# 4.11 Generate a script to upload results to Google Drive database
if (counter == 0) {
generate_script(dirname(metadata_main))
}
# 4.12 Save all features inside the input folder
bandnames <- c(paste0("B",1:8), "B8A", paste0("B", 9:12), "CDI", "VV", "VH", "angle", "elevation", "landuse", "cloudshadow_direction")
# 4.14 Save input values
input_data <- raster::stack(
final_stack[[1:13]]/10000, final_stack[[14]], final_stack[[15:17]], final_stack[[22:24]]
)
input_spec <- sprintf("%s/input/%s.tif", output_final_folder, bandnames)
lapply(1:20, function(x) writeRaster(input_data[[x]], input_spec[x], overwrite = TRUE))
# 4.15 Save target values
# 18-19 -> cmask_s2cloudness| cmask_s2cloudness_reclass (0,1)
# 20-21 -> cmask_sen2cor | cmask_sen2cor_reclass (0,1,2)
# 25 -> IPL_cloudmask_reclass
create_target_raster(
final_stack = final_stack,
IPL_multitemporal_cloud_logical = IPL_multitemporal_cloud_logical,
output_final_folder = output_final_folder
)
# 4.16 Create metadata.json for each file
ee_create_metadata(
id = basename(s2_id),
point = c(jsonfile_r$y, jsonfile_r$x),
path = metadata_spec
)
# 4.17 Create a thumbnail
ee_generate_thumbnail(
s2_id = s2_id,
final_stack =  final_stack,
crs_kernel =  crs_kernel,
output_final_folder = output_final_folder
)
counter <- counter + 1
}
row_position <- gsub("point_", "", point_name) %>% as.numeric()
# point_metadata
df_final <- data_frame(
id = sprintf("%s_%02d", point_name, 1:5),
labeler = sp_db[row_position,]$labeler,
type = sp_db[row_position,]$label,
difficulty = NA,
sen2_id = s2_ids,
sen2_date = s2_dates,
sen1_id = s1_ids,
s1_date = s1_dates,
sen1_area = s1_area_per,
land_use = land_use,
elevation = elevation,
shadow_dir = shadow_dir,
split = NA,
state = FALSE,
evaluation_I = sp_db[row_position,]$validator,
evaluation_II = labelers_names[!(labelers_names %in% c(sp_db[row_position,]$validator, sp_db[row_position,]$labeler))],
evaluation_Expert = FALSE
)
write_csv(
x = df_final,
file = sprintf("%s/%s_metadata.csv", dirname(metadata_main), point_name)
)
# 5. Save geometry
roi <- extent(final_stack[[1]]) %>%
st_bbox() %>%
st_as_sfc()
st_crs(roi) <- crs_kernel
write_sf(roi, sprintf("%s/%s.gpkg", dirname(output_final_folder), point_name))
}
